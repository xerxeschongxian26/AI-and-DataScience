{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Dataset and Library Loading"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nplt.style.use('ggplot')            #popular R style of plots\n#print(plt.style.available)   To view all available styles\nfrom collections import Counter\n#Sci-Kit Library\nfrom sklearn.utils import shuffle\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\n#Pytorch\nimport torch\nimport torch.nn as nn\nfrom torch.nn import functional as F\nfrom torch.autograd import Variable\n\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n# import os\n# for dirname, _, filenames in os.walk('/kaggle/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\ndf_train = pd.read_csv('../input/titanic/train.csv')\ndf_test  = pd.read_csv('../input/titanic/test.csv')\ndf_sub   = pd.read_csv('../input/titanic/gender_submission.csv')\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Preprocessing dataset for model\n\n* Drop redundant columns\n* One hot encoding categorical variables\n* Impute (filling in missing data using suitable value) necessary columns \n* Feature Engineering\n     *     Combining SibSp and Parch into a single Family variable\n     *     Drop the redundant SibSp and Parch\n* Scale both train and test data for linear models\n* Randomly shuffle dataset, split data for train and test"},{"metadata":{"trusted":true},"cell_type":"code","source":"# df_train.info()\n#Useless columns: PassengerId(similar to the index),Name(self explanatory), Ticket, Cabin(too many null values)\ndf_train.drop(['PassengerId','Name','Ticket','Cabin'],axis=1,inplace=True)\ndf_test.drop(['PassengerId','Name','Ticket','Cabin'],axis=1,inplace=True)\n\n\n#Sex and Embarked are categorical data that must be one hot encoded for the model to process it\n#drop_first=True to reduce size of encoded data\n#Original columns are not redundant and can be dropped and encoded data is concatenated back into dataframe\nsex=pd.get_dummies(df_train['Sex'],drop_first=True)\nembark=pd.get_dummies(df_train['Embarked'],drop_first=True)\ndf_train=pd.concat([df_train,sex,embark],axis=1)\ndf_train.drop(['Sex','Embarked'],axis=1,inplace=True)\n\nsex=pd.get_dummies(df_test['Sex'],drop_first=True)\nembark=pd.get_dummies(df_test['Embarked'],drop_first=True)\ndf_test=pd.concat([df_test,sex,embark],axis=1)\ndf_test.drop(['Sex','Embarked'],axis=1,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"#df_train.info()\n#Age has null values still unfilled\n#df_test.info()\n#Age and Fare has null values still unfilled\ndf_train.fillna(df_train.mean(),inplace=True)\ndf_test.fillna(df_test.mean(),inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train['Fam']=df_train['SibSp']+df_train['Parch']\ndf_train['Family']=df_train['Fam']\ndf_train.loc[df_train['Family'] > 0, 'Family'] = 1 \ndf_train.loc[df_train['Family'] == 0, 'Family'] = 0 \ndf_train.drop(['SibSp','Parch','Fam'],axis=1,inplace=True)\n\ndf_test['Fam']=df_test['SibSp']+df_test['Parch']\ndf_test['Family']=df_test['Fam']\ndf_test.loc[df_test['Family'] > 0, 'Family'] = 1 \ndf_test.loc[df_test['Family'] == 0, 'Family'] = 0 \ndf_test.drop(['SibSp','Parch','Fam'],axis=1,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Scale the data\nScaler1 = StandardScaler()   #instantiate StandardScalar object\nScaler2 = StandardScaler()\n\ntrain_columns=df_train.columns\ntest_columns=df_test.columns\n \ndf_train = pd.DataFrame(Scaler1.fit_transform(df_train))    #why do we need to instantiate 2 instances for the train and test\ndf_test  = pd.DataFrame(Scaler2.fit_transform(df_test))     #Scaling removes the column headers and we have to reassign them \n\ndf_train.columns=train_columns\ndf_test.columns=test_columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#features=df_train.iloc[:,2:] grabs all the rows but columns starting from 2 onwards\n#features=df_train.iloc[:,2:].columns returns an object that contains the columns headers\n#features=df_train.iloc[:,2:].columns.tolist() returns a list that contains the columns headers\n#target=df_train.loc[:,'Survived'] returns the series of Survived\n#target=df_train.loc[:,'Survived'].name returns the name of the series  which is a string called \"Survived\"\nfeatures=df_train.iloc[:,1:].columns.tolist()                    \ntarget=df_train.loc[:,'Survived'].name\n\n#Shuffle rows, drop the new index that is automatically created\ndf_train=df_train.sample(frac=1,axis=0).reset_index(drop=True)\ndf_train=df_train.sample(frac=1,axis=0).reset_index(drop=True)\n\nX_train = df_train.iloc[:,1:].values\ny_train = df_train.loc[:, 'Survived'].values\n\n#7 Features we are training on are ['Pclass', 'Age', 'SibSp', 'Parch', 'Fare', 'male', 'Q', 'S']\n#y value is the values of Survived","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# PyTorch Logistic Regression \n\n\n* Model Definition and Instantiation \n* Loss Function\n* Optimizer Function\n* Training\n* Prediction\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Define the NN model class object\nclass Net(nn.Module):\n    def __init__(self):\n        \n        super(Net,self).__init__()           #No inputs have been defined for __init__(), hence the NN parameters are defined by default\n        self.fc1=nn.Linear(7,512)            #4 Fully connected layers with 2 hidden layers\n        self.fc2=nn.Linear(512,512)\n        self.fc3=nn.Linear(512,2)\n        self.dropout=nn.Dropout(0.5)\n        \n        # Input layer(7 features)-->512 Neurons-->512 Neurons--> Output (2 classes)\n        #Input---_------------------Drop out   -->Drop out   --> Output   \n        #Dropout rate of neurons at 0.5\n        \n    def forward(self,x):                     #Forward Propagation\n        x=F.relu(self.fc1(x))                #Input --> Hidden 1\n        x=self.dropout(x)                    #Dropout Hidden 1\n        x=F.relu(self.fc2(x))                #Hidden 1 --> Hidden 2\n        x=self.dropout(x)                    #Dropout Hidden 2\n        x=self.fc3(x)                        #Hidden 2 --> Output\n        return x\n    \n#Instantiate an NN object and display param\nmodel=Net()\nprint(model)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"criterion=nn.CrossEntropyLoss()                         #For Logistic regression problem\noptimizer=torch.optim.SGD(model.parameters(),lr=0.06)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Mini-batch gradient descent is performed here. Batch size of 64\nbatch_size=9\nn_epochs=500\nbatch_no=len(X_train) // batch_size                            #X_train has length 891\n\nlosses=[]\ntrain_loss=0\ntrain_loss_min=np.Inf\n\nfor epoch in range(n_epochs):                                    #A single epoch is a single pass over the entire input X_train\n    for i in range(batch_no):           \n        \n        #Define mini batches\n        start = i*batch_size\n        end = start+batch_size\n        x_var = Variable(torch.FloatTensor(X_train[start:end]))  \n        y_var = Variable(torch.LongTensor(y_train[start:end])) \n        \n        #Training and Back propagation\n        optimizer.zero_grad()\n        output=model(x_var)\n        loss=criterion(output,y_var)\n        loss.backward()\n        optimizer.step()\n        \n        #Calculating the accuracy\n        values,labels =torch.max(output,1)\n        num_right = np.sum(labels.data.numpy() == y_var.numpy())    #y_train[start:end] was changed to y_var[start:end]\n        train_loss += loss.item()*batch_size                             #sum up losses per batch multiplied by batch_size(64)\n        \n    train_loss = train_loss / len(X_train)                          #train_loss for that epoch is averaged by the length of X_train\n    \n    if train_loss <= train_loss_min:\n        \n#       print(\"Validation loss decreased ({:6f} ===> {:6f}). Saving the model...\".format(train_loss_min,train_loss))\n#       torch.save(model.state_dict(), \"model.pt\")\n        print(\"Validation loss decreased ({:6f} ===> {:6f}) from Epoc {} to {}\".format(train_loss_min,train_loss,epoch-1,epoch))\n        train_loss_min = train_loss\n        \n    if epoch % 200==0:\n        \n        print('')\n        print(\"Epoch: {} \\tTrain Loss: {} \\tTrain Accuracy: {}\".format(epoch+1, train_loss,num_right / len(y_train[start:end]) ))\n        \n    losses.append(train_loss)\nprint('\\nTraining Ended!\\nFinal training loss is : {} \\nFinal Accuracy is : {}'.format(losses[-1],num_right / len(y_train[start:end])))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Plot the losses\nfig, axes = plt.subplots(figsize=(12,3),dpi=200)\n\naxes.plot(range(0,n_epochs), losses, 'r')\naxes.set_xlabel('Epoch')\naxes.set_ylabel('Cross Entropy Loss')\n\nfig.savefig(\"Training Loss.png\", dpi=200)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# X_test=df_test.iloc[:,1:].values\nX_test=df_test.values\nX_test_var = Variable(torch.FloatTensor(X_test), requires_grad=False) \nwith torch.no_grad():           #Turn off the gradient update\n    test_result=model(X_test_var)\nvalues, labels=torch.max(test_result,1)\nsurvived=labels.data.numpy()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Submission"},{"metadata":{"trusted":true},"cell_type":"code","source":"submission=pd.DataFrame({'PassengerId':df_sub['PassengerId'],'Survived':survived})\nsubmission.to_csv('submission.csv',index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train[0:5]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"values,label=torch.max(output,1)\nlabel.data.numpy==y_train[start:end]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"output","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"label.data.numpy","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_var = Variable(torch.LongTensor(y_train[0:64])) \ny_var.numpy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"num_right = np.sum(labels.data.numpy() == y_train[start:end])\nlen(labels.data.numpy())\nlen(y_train[start:end])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = {'day': ['Mon', 'Tues', 'Wed', 'Thurs', 'Fri'],\n       'color': ['Blue', 'Red', 'Green', 'Yellow', 'Black'],\n       'Number': [11, 8, 10, 15, 11]}\n\ndataframe = pd.DataFrame(data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataframe","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataframe=dataframe.sample(frac=1,axis=0).reset_index(drop=True)\ndataframe","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataframe","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}